{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes (1/3) - From scratch\n",
    "\n",
    "This post explores some concepts behind Gaussian processes, such as stochastic processes and the kernel function. We will build up deeper understanding of Gaussian process regression by implementing them from scratch using Python and NumPy.\n",
    "\n",
    "This post is followed by [a second post demonstrating how to fit a Gaussian process kernel ]({% post_url /blog/gaussian_process/2019-01-06-gaussian-process-kernel-fitting %}) with [TensorFlow probability](https://www.tensorflow.org/probability/). This is the first post part of a series on Gaussian processes:\n",
    "\n",
    "1. [Understanding Gaussian processes (this)]({% post_url /blog/gaussian_process/2019-01-05-gaussian-process-tutorial %})\n",
    "2. [Fitting a Gaussian process kernel]({% post_url /blog/gaussian_process/2019-01-06-gaussian-process-kernel-fitting %})\n",
    "3. [Gaussian process kernels]({% post_url /blog/gaussian_process/2019-01-07-gaussian-process-kernels %})\n",
    "\n",
    "In what follows we assume familiarity with basic probability and linear algebra especially in the context of multivariate Gaussian distributions. Have a look at [this post]({% post_url /blog/misc/2018-09-28-multivariate-normal-primer %}) if you need a refresher on the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# Set matplotlib and seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "np.random.seed(42)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "What are Gaussian processes? The name implies that it's a stochastic process of random variables with a Gaussian distribution. This might not mean much at this moment so lets dig a bit deeper into its meaning.\n",
    "\n",
    "\n",
    "## Stochastic process\n",
    "\n",
    "[Stochastic processes](https://en.wikipedia.org/wiki/Stochastic_process) typically describe systems randomly changing over time. The processes are [stochastic](https://en.wikipedia.org/wiki/Stochastic) due to the uncertainty in the system. Even if the starting point is known, there are several directions in which the processes can evolve.\n",
    "\n",
    "An example of a stochastic process that you might have come across is the model of [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion) (also known as [Wiener process](https://en.wikipedia.org/wiki/Wiener_process) ). Brownian motion is the random motion of particles suspended in a fluid. It can be seen as a continuous [random walk](https://en.wikipedia.org/wiki/Random_walk) where a particle moves around in the fluid due to other particles randomly bumping into it. We can simulate this process over time $t$ in 1 dimension $d$ by starting out at position 0 and moving the particle over a certain amount of time $\\Delta t$ with a random distance $\\Delta d$ from the previous position. The random distance is sampled from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean $0$ and variance $\\Delta t$. Sampling $\\Delta d$ from this normal distribution is noted as $\\Delta d \\sim \\mathcal{N}(0, \\Delta t)$. The position $d(t)$ at time $t$ evolves as $d(t + \\Delta t) = d(t) + \\Delta d$.\n",
    "\n",
    "We simulate 5 different paths of Brownian motion in the following figure, each path is illustrated with a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D simulation of the Brownian motion process\n",
    "total_time = 1\n",
    "nb_steps = 75\n",
    "delta_t = total_time / nb_steps\n",
    "nb_processes = 5  # Simulate 5 different motions\n",
    "mean = 0.  # Mean of each movement\n",
    "stdev = np.sqrt(delta_t)  # Standard deviation of each movement\n",
    "\n",
    "# Simulate the brownian motions in a 1D space by cumulatively\n",
    "#  making a new movement delta_d\n",
    "distances = np.cumsum(\n",
    "    # Move randomly from current location to N(0, delta_t)\n",
    "    np.random.normal(\n",
    "        mean, stdev, (nb_processes, nb_steps)),\n",
    "    axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Make the plots\n",
    "t = np.arange(0, total_time, delta_t)\n",
    "for i in range(nb_processes):\n",
    "    plt.plot(t, distances[i,:])\n",
    "plt.title((\n",
    "    'Brownian motion process\\n '\n",
    "    'Position over time for 5 independent realizations'))\n",
    "plt.xlabel('$t$ (time)', fontsize=13)\n",
    "plt.ylabel('$d$ (position)', fontsize=13)\n",
    "plt.xlim([-0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stochastic processes as distributions over functions\n",
    "\n",
    "Notice in the figure above that the stochastic process can lead to different paths, also known as [realizations](https://en.wikipedia.org/wiki/Realization_%28probability%29) of the process.  Each realization defines a position $d$ for every possible timestep $t$. Every realization thus corresponds to a function $f(t) = d$.\n",
    "\n",
    "This means that a stochastic process can be interpreted as a random distribution over functions. We can sample a realization of a function from a stochastic process. However each realized function can be different due to the randomness of the stochastic process.\n",
    "\n",
    "Like the model of Brownian motion, Gaussian processes are stochastic processes. In fact, the Brownian motion process can be reformulated as a Gaussian process[⁽³⁾](#References)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gaussian processes\n",
    "\n",
    "Gaussian processes are distributions over functions $f(x)$ of which the distribution is defined by a mean function $m(x)$ and [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix) covariance function $k(x,x')$, with $x$ the function values and $(x,x')$ all possible pairs in the input [domain](https://en.wikipedia.org/wiki/Domain_of_a_function):\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x),k(x,x'))$$\n",
    "\n",
    "where for any finite subset $X =\\{\\mathbf{x}_1 \\ldots \\mathbf{x}_n \\}$ of the domain of $x$, the [marginal distribution](https://en.wikipedia.org/wiki/Marginal_distribution) is a [multivariate Gaussian]({% post_url /blog/misc/2018-09-28-multivariate-normal-primer %}) distribution:\n",
    "\n",
    "$$f(X) \\sim \\mathcal{N}(m(X), k(X, X))$$\n",
    "\n",
    "with mean vector $\\mathbf{\\mu} = m(X)$ and covariance matrix $\\Sigma = k(X, X)$.\n",
    "\n",
    "While the multivariate Gaussian captures a finite number of jointly distributed Gaussians, the Gaussian process doesn't have this limitation. Its mean and covariance are defined by a [function](https://en.wikipedia.org/wiki/Function_\\(mathematics\\)). Each input to this function is a variable correlated with the other variables in the input domain, as defined by the covariance function. Since functions can have an infinite input domain, the Gaussian process can be interpreted as an infinite dimensional Gaussian random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Covariance function as prior\n",
    "\n",
    "To sample functions from the Gaussian process we need to define the mean and covariance functions. The [covariance function](https://en.wikipedia.org/wiki/Covariance_function) $k(x_a, x_b)$ models the joint variability of the Gaussian process random variables. It returns the modelled [covariance](https://en.wikipedia.org/wiki/Covariance) between each pair in $x_a$ and $x_b$.\n",
    "\n",
    "The specification of this covariance function, also known as the kernel function, implies a distribution over functions $f(x)$. By choosing a specific kernel function $k$ it is possible to set [prior](https://en.wikipedia.org/wiki/Prior_probability) information on this distribution. This kernel function needs to be [positive-definite](https://en.wikipedia.org/wiki/Positive-definite_function) in order to be a valid covariance function.\n",
    "\n",
    "In this post we will model the covariance with the [exponentiated quadratic ](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) covariance function (also known as the RBF kernel):\n",
    "\n",
    "$$k(x_a, x_b) = \\exp{ \\left( -\\frac{1}{2\\sigma^2} \\lVert x_a - x_b \\rVert^2 \\right)}$$\n",
    "\n",
    "Other kernel functions can be defined resulting in different priors on the Gaussian process distribution. Examples of different kernels are given in a [follow-up post]({% post_url /blog/gaussian_process/2019-01-07-gaussian-process-kernels %})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exponentiated quadratic \n",
    "def exponentiated_quadratic(xa, xb):\n",
    "    \"\"\"Exponentiated quadratic  with σ=1\"\"\"\n",
    "    # L2 distance (Squared Euclidian)\n",
    "    sq_norm = -0.5 * scipy.spatial.distance.cdist(xa, xb, 'sqeuclidean')\n",
    "    return np.exp(sq_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example covariance matrix from the exponentiated quadratic  covariance function is plotted in the figure below on the left. The covariance vs input zero is plotted on the right. Note that the exponentiated quadratic covariance decreases exponentially the further away the function values $x$ are from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate covariance matrix and function\n",
    "\n",
    "# Show covariance matrix example from exponentiated quadratic\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "xlim = (-3, 3)\n",
    "X = np.expand_dims(np.linspace(*xlim, 25), 1)\n",
    "Σ = exponentiated_quadratic(X, X)\n",
    "# Plot covariance matrix\n",
    "im = ax1.imshow(Σ, cmap=cm.YlGnBu)\n",
    "cbar = plt.colorbar(\n",
    "    im, ax=ax1, fraction=0.045, pad=0.05)\n",
    "cbar.ax.set_ylabel('$k(x,x)$', fontsize=10)\n",
    "ax1.set_title((\n",
    "    'Exponentiated quadratic \\n'\n",
    "    'example of covariance matrix'))\n",
    "ax1.set_xlabel('x', fontsize=13)\n",
    "ax1.set_ylabel('x', fontsize=13)\n",
    "ticks = list(range(xlim[0], xlim[1]+1))\n",
    "ax1.set_xticks(np.linspace(0, len(X)-1, len(ticks)))\n",
    "ax1.set_yticks(np.linspace(0, len(X)-1, len(ticks)))\n",
    "ax1.set_xticklabels(ticks)\n",
    "ax1.set_yticklabels(ticks)\n",
    "ax1.grid(False)\n",
    "\n",
    "# Show covariance with X=0\n",
    "xlim = (-4, 4)\n",
    "X = np.expand_dims(np.linspace(*xlim, num=50), 1)\n",
    "zero = np.array([[0]])\n",
    "Σ0 = exponentiated_quadratic(X, zero)\n",
    "# Make the plots\n",
    "ax2.plot(X[:,0], Σ0[:,0], label='$k(x,0)$')\n",
    "ax2.set_xlabel('x', fontsize=13)\n",
    "ax2.set_ylabel('covariance', fontsize=13)\n",
    "ax2.set_title((\n",
    "    'Exponentiated quadratic  covariance\\n'\n",
    "    'between $x$ and $0$'))\n",
    "# ax2.set_ylim([0, 1.1])\n",
    "ax2.set_xlim(*xlim)\n",
    "ax2.legend(loc=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sampling from prior\n",
    "\n",
    "In practice we can't just sample a full function evaluation $f$ from a Gaussian process distribution since that would mean evaluating $m(x)$ and $k(x,x')$ at an infinite number of points since $x$ can have an infinite [domain](https://en.wikipedia.org/wiki/Domain_of_a_function). We can however sample function evaluations $\\mathbf{y}$ of a function $f$ drawn from a Gaussian process at a finite, but arbitrary, set of points $X$: $\\mathbf{y} = f(X)$.\n",
    "\n",
    "A finite dimensional subset of the Gaussian process distribution results in a [marginal distribution](https://en.wikipedia.org/wiki/Marginal_distribution) that is a Gaussian distribution $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\Sigma)$ with mean vector $\\mathbf{\\mu} = m(X)$, covariance matrix $\\Sigma = k(X, X)$. \n",
    "\n",
    "In the figure below we will sample 5 different function realisations from a Gaussian process with exponentiated quadratic prior[⁽¹⁾](#Sidenotes) without any observed data. We do this by drawing correlated samples from a 41-dimensional Gaussian $\\mathcal{N}(0, k(X, X))$ with $X = [X_1, \\ldots, X_{41}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the Gaussian process distribution\n",
    "nb_of_samples = 41  # Number of points in each function\n",
    "number_of_functions = 5  # Number of functions to sample\n",
    "# Independent variable samples\n",
    "X = np.expand_dims(np.linspace(-4, 4, nb_of_samples), 1)\n",
    "Σ = exponentiated_quadratic(X, X)  # Kernel of data points\n",
    "\n",
    "# Draw samples from the prior at our data points.\n",
    "# Assume a mean of 0 for simplicity\n",
    "ys = np.random.multivariate_normal(\n",
    "    mean=np.zeros(nb_of_samples), cov=Σ, \n",
    "    size=number_of_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sampled functions\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(number_of_functions):\n",
    "    plt.plot(X, ys[i], linestyle='-', marker='o', markersize=3)\n",
    "plt.xlabel('$x$', fontsize=13)\n",
    "plt.ylabel('$y = f(x)$', fontsize=13)\n",
    "plt.title((\n",
    "    '5 different function realizations at 41 points\\n'\n",
    "    'sampled from a Gaussian process with exponentiated quadratic kernel'))\n",
    "plt.xlim([-4, 4])\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualise this is to take only 2 dimensions of this 41-dimensional Gaussian and plot some of it's 2D marginal distributions.\n",
    "\n",
    "The next figure on the left visualizes the 2D distribution for $X = [0, 0.2]$ where the covariance $k(0, 0.2) = 0.98$. The figure on the right visualizes the 2D distribution for $X = [0, 2]$ where the covariance $k(0, 2) = 0.14$.\n",
    "\n",
    "For each of the 2D Gaussian marginals the corresponding samples from the function realisations above have been plotted as colored dots on the figure.\n",
    "\n",
    "Observe that points close together in the input domain of $x$ are strongly correlated ($y_1$ is close to $y_2$), while points further away from each other are almost independent. This is because these marginals come from a Gaussian process with as prior the exponentiated quadratic covariance, which adds prior information that points close to each other in the input space $X$ must be close to each other in the output space $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show marginal 2D Gaussians\n",
    "\n",
    "def generate_surface(mean, covariance, surface_resolution):\n",
    "    \"\"\"Helper function to generate density surface.\"\"\"\n",
    "    x1s = np.linspace(-5, 5, num=surface_resolution)\n",
    "    x2s = np.linspace(-5, 5, num=surface_resolution)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s) # Generate grid\n",
    "    pdf = np.zeros((surface_resolution, surface_resolution))\n",
    "    # Fill the cost matrix for each combination of weights\n",
    "    for i in range(surface_resolution):\n",
    "        for j in range(surface_resolution):\n",
    "            pdf[i,j] = scipy.stats.multivariate_normal.pdf(\n",
    "                np.array([x1[i,j], x2[i,j]]), \n",
    "                mean=mean, cov=covariance)\n",
    "    return x1, x2, pdf  # x1, x2, pdf(x1,x2)\n",
    "\n",
    "surface_resolution = 50  # Resolution of the surface to plot\n",
    "fig = plt.figure(figsize=(6.2, 3.5)) \n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "ax_p1 = plt.subplot(gs[0,0])\n",
    "ax_p2 = plt.subplot(gs[0,1], sharex=ax_p1, sharey=ax_p1)\n",
    "\n",
    "# Plot of strong correlation\n",
    "X_strong = np.array([[0], [0.2]])\n",
    "μ = np.array([0., 0.])\n",
    "Σ_strong = exponentiated_quadratic(X_strong, X_strong)\n",
    "y1, y2, p = generate_surface(μ, Σ_strong, surface_resolution=surface_resolution)\n",
    "# Plot bivariate distribution\n",
    "con1 = ax_p1.contourf(y1, y2, p, 25, cmap=cm.magma_r)\n",
    "ax_p1.set_xlabel(\n",
    "    f'$y_1 = f(X={X_strong[0,0]})$', \n",
    "    fontsize=11, labelpad=0)\n",
    "ax_p1.set_ylabel(\n",
    "    f'$y_2 = f(X={X_strong[1,0]})$', \n",
    "    fontsize=11)\n",
    "ax_p1.axis([-2.7, 2.7, -2.7, 2.7])\n",
    "ax_p1.set_aspect('equal')\n",
    "ax_p1.text(\n",
    "    -2.3, 2.1, \n",
    "    (f'$k({X_strong[0,0]}, {X_strong[1,0]}) '\n",
    "     f'= {Σ_strong[0,1]:.2f}$'), \n",
    "    fontsize=10)\n",
    "ax_p1.set_title(\n",
    "    f'$X = [{X_strong[0,0]}, {X_strong[1,0]}]$ ', \n",
    "    fontsize=12)\n",
    "# Select samples\n",
    "X_0_index = np.where(np.isclose(X, 0.))\n",
    "X_02_index = np.where(np.isclose(X, 0.2))\n",
    "y_strong = ys[:,[X_0_index[0][0], X_02_index[0][0]]]\n",
    "# Show samples on surface\n",
    "for i in range(y_strong.shape[0]):\n",
    "    ax_p1.plot(y_strong[i,0], y_strong[i,1], marker='o')\n",
    "\n",
    "# Plot weak correlation\n",
    "X_weak = np.array([[0], [2]])\n",
    "μ = np.array([0., 0.])\n",
    "Σ_weak = exponentiated_quadratic(X_weak, X_weak)\n",
    "y1, y2, p = generate_surface(μ, Σ_weak, surface_resolution=surface_resolution)\n",
    "# Plot bivariate distribution\n",
    "con2 = ax_p2.contourf(y1, y2, p, 25, cmap=cm.magma_r)\n",
    "con2.set_cmap(con1.get_cmap())\n",
    "con2.set_clim(con1.get_clim())\n",
    "ax_p2.set_xlabel(\n",
    "    f'$y_1 = f(X={X_weak[0,0]})$', \n",
    "    fontsize=11, labelpad=0)\n",
    "ax_p2.set_ylabel(\n",
    "    f'$y_2 = f(X={X_weak[1,0]})$', \n",
    "    fontsize=11)\n",
    "ax_p2.set_aspect('equal')\n",
    "ax_p2.text(\n",
    "    -2.3, 2.1, \n",
    "    (f'$k({X_weak[0,0]}, {X_weak[1,0]}) '\n",
    "     f'= {Σ_weak[0,1]:.2f}$'), \n",
    "    fontsize=10)\n",
    "ax_p2.set_title(\n",
    "    f'$X = [{X_weak[0,0]}, {X_weak[1,0]}]$', \n",
    "    fontsize=12)\n",
    "# Add colorbar\n",
    "divider = make_axes_locatable(ax_p2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.02)\n",
    "cbar = plt.colorbar(con1, ax=ax_p2, cax=cax)\n",
    "cbar.ax.set_ylabel('density: $p(y_1, y_2)$', fontsize=11)\n",
    "fig.suptitle(r'2D marginal: $y \\sim \\mathcal{N}(0, k(X, X))$')\n",
    "# Select samples\n",
    "X_0_index = np.where(np.isclose(X, 0.))\n",
    "X_2_index = np.where(np.isclose(X, 2.))\n",
    "y_weak = ys[:,[X_0_index[0][0], X_2_index[0][0]]]\n",
    "# Show samples on surface\n",
    "for i in range(y_weak.shape[0]):\n",
    "    ax_p2.plot(y_weak[i,0], y_weak[i,1], marker='o')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gaussian processes for regression\n",
    "\n",
    "Since Gaussian processes model distributions over functions we can use them to build [regression](https://en.wikipedia.org/wiki/Regression_analysis) models. We can treat the Gaussian process as a prior defined by the kernel function and create a [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) given some data. This posterior distribution can then be used to predict the expected value and probability of the output variable $\\mathbf{y}$ given input variables $X$.\n",
    "\n",
    "\n",
    "### Predictions from posterior\n",
    "\n",
    "We want to make predictions $\\mathbf{y}_2 = f(X_2)$ for $n_2$ new samples, and we want to make these predictions based on our Gaussian process prior and $n_1$ previously observed data points $(X_1,\\mathbf{y}_1)$. This can be done with the help of the posterior distribution $p(\\mathbf{y}_2 \\mid \\mathbf{y}_1,X_1,X_2)$. Keep in mind that $\\mathbf{y}_1$ and $\\mathbf{y}_2$ are [jointly Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Joint_normality) since they both come from the same multivariate distribution. Since they are jointly Gaussian and we have a finite number of samples we can write: \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} \\mathbf{y}_{1} \\\\ \\mathbf{y}_{2} \\end{array}\\right]\n",
    "\\sim\n",
    "\\mathcal{N} \\left(\n",
    "\\left[\\begin{array}{c} \\mu_{1} \\\\ \\mu_{2} \\end{array}\\right],\n",
    "\\left[ \\begin{array}{cc}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{array} \\right]\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\\begin{split}\n",
    "\\mu_{1} & = m(X_1) \\quad (n_1 \\times 1) \\\\\n",
    "\\mu_{2} & = m(X_2) \\quad (n_2 \\times 1) \\\\\n",
    "\\Sigma_{11} & = k(X_1,X_1) \\quad (n_1 \\times n_1) \\\\\n",
    "\\Sigma_{22} & = k(X_2,X_2) \\quad (n_2 \\times n_2) \\\\\n",
    "\\Sigma_{12} & = k(X_1,X_2) = k_{21}^\\top \\quad (n_1 \\times n_2)\n",
    "\\end{split}$$\n",
    "\n",
    "Note that $\\Sigma_{11}$ is independent of $\\Sigma_{22}$ and vice versa.\n",
    "\n",
    "We can then get the [conditional distribution]({% post_url /blog/misc/2018-09-28-multivariate-normal-primer %}#Conditional-distribution):\n",
    "\n",
    "$$\\begin{split}\n",
    "p(\\mathbf{y}_2 \\mid \\mathbf{y}_1, X_1, X_2) & =  \\mathcal{N}(\\mu_{2|1}, \\Sigma_{2|1}) \\\\\n",
    "\\mu_{2|1} & = \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{y}_1 - \\mu_1) \\\\\n",
    "          & = \\Sigma_{21} \\Sigma_{11}^{-1} \\mathbf{y}_1 \\quad (\\text{if assume mean prior }  \\mu = 0 ) \\\\\n",
    "\\Sigma_{2|1} & = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1}\\Sigma_{12}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these as follows (Note here that $\\Sigma_{11} = \\Sigma_{11}^{\\top}$ since it's [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix).):\n",
    "\n",
    "$$\\begin{array}{cc}\n",
    "\\begin{split}\n",
    "\\mu_{2|1} & = \\Sigma_{21} \\Sigma_{11}^{-1} \\mathbf{y}_1 \\\\\n",
    "          & = (\\Sigma_{11}^{-1} \\Sigma_{12})^{\\top}  \\mathbf{y}_1 \\\\\n",
    "\\end{split}\n",
    "& \\qquad\n",
    "\\begin{split}\n",
    "\\Sigma_{2|1} & = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\\\\n",
    "             & = \\Sigma_{22} - (\\Sigma_{11}^{-1} \\Sigma_{12})^{\\top} \\Sigma_{12} \\\\\n",
    "\\end{split}\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "\n",
    "It is then possible to predict $\\mathbf{y}_2$ corresponding to the input samples $X_2$ by using the mean $\\mu_{2|1}$ of the resulting distribution as a prediction. \n",
    "Notice that the mean of the posterior predictions $\\mu_{2|1}$ of a Gaussian process are weighted averages of the observed variables $\\mathbf{y}_1$, where the weighting is based on the covariance function $k$. The variance $\\sigma_2^2$ of these predictions is then the diagonal of the covariance matrix $\\Sigma_{2|1}$.\n",
    "\n",
    "The Gaussian process posterior is implemented in the `GP` method below. We can compute the $\\Sigma_{11}^{-1} \\Sigma_{12}$ term with the help of Scipy's [`solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html) function[⁽²⁾](#Sidenotes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process posterior\n",
    "def GP(X1, y1, X2, kernel_func):\n",
    "    \"\"\"\n",
    "    Calculate the posterior mean and covariance matrix for y2\n",
    "    based on the corresponding input X2, the observations (y1, X1), \n",
    "    and the prior kernel function.\n",
    "    \"\"\"\n",
    "    # Kernel of the observations\n",
    "    Σ11 = kernel_func(X1, X1)\n",
    "    # Kernel of observations vs to-predict\n",
    "    Σ12 = kernel_func(X1, X2)\n",
    "    # Solve\n",
    "    solved = scipy.linalg.solve(Σ11, Σ12, assume_a='pos').T\n",
    "    # Compute posterior mean\n",
    "    μ2 = solved @ y1\n",
    "    # Compute the posterior covariance\n",
    "    Σ22 = kernel_func(X2, X2)\n",
    "    Σ2 = Σ22 - (solved @ Σ12)\n",
    "    return μ2, Σ2  # mean, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the posterior distribution based on 8 observations from a sine function. The results are plotted below. The top figure shows the distribution where the red line is the posterior mean, the shaded area is the 95% prediction interval, the black dots are the observations $(X_1,\\mathbf{y}_1)$.\n",
    "The prediction interval is computed from the standard deviation $\\sigma_{2|1}$, which is the square root of the diagonal of the covariance matrix. The bottom figure shows 5 realizations (sampled functions) from this distribution.  \n",
    "\n",
    "Note that the distribution is quite confident of the points predicted around the observations $(X_1,\\mathbf{y}_1)$, and that the prediction interval gets larger the further away it is from these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the posterior mean and covariance\n",
    "\n",
    "# Define the true function that we want to regress on\n",
    "f_sin = lambda x: (np.sin(x)).flatten()\n",
    "\n",
    "n1 = 8  # Number of points to condition on (training points)\n",
    "n2 = 75  # Number of points in posterior (test points)\n",
    "ny = 5  # Number of functions that will be sampled from the posterior\n",
    "domain = (-6, 6)\n",
    "\n",
    "# Sample observations (X1, y1) on the function\n",
    "X1 = np.random.uniform(domain[0]+2, domain[1]-2, size=(n1, 1))\n",
    "y1 = f_sin(X1)\n",
    "# Predict points at uniform spacing to capture function\n",
    "X2 = np.linspace(domain[0], domain[1], n2).reshape(-1, 1)\n",
    "# Compute posterior mean and covariance\n",
    "μ2, Σ2 = GP(X1, y1, X2, exponentiated_quadratic)\n",
    "# Compute the standard deviation at the test points to be plotted\n",
    "σ2 = np.sqrt(np.diag(Σ2))\n",
    "\n",
    "# Draw some samples of the posterior\n",
    "y2 = np.random.multivariate_normal(mean=μ2, cov=Σ2, size=ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the postior distribution and some samples\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(6, 6))\n",
    "# Plot the distribution of the function (mean, covariance)\n",
    "ax1.plot(X2, f_sin(X2), 'b--', label='$sin(x)$')\n",
    "ax1.fill_between(X2.flat, μ2-2*σ2, μ2+2*σ2, color='red', \n",
    "                 alpha=0.15, label=r'$2 \\sigma_{2|1}$')\n",
    "ax1.plot(X2, μ2, 'r-', lw=2, label=r'$\\mu_{2|1}$')\n",
    "ax1.plot(X1, y1, 'ko', linewidth=2, label='$(x_1, y_1)$')\n",
    "ax1.set_xlabel('$x$', fontsize=13)\n",
    "ax1.set_ylabel('$y$', fontsize=13)\n",
    "ax1.set_title('Distribution of posterior and prior data.')\n",
    "ax1.axis([domain[0], domain[1], -3, 3])\n",
    "ax1.legend()\n",
    "# Plot some samples from this function\n",
    "ax2.plot(X2, y2.T, '-')\n",
    "ax2.set_xlabel('$x$', fontsize=13)\n",
    "ax2.set_ylabel('$y$', fontsize=13)\n",
    "ax2.set_title('5 different function realizations from posterior')\n",
    "ax1.axis([domain[0], domain[1], -3, 3])\n",
    "ax2.set_xlim([-6, 6])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Noisy observations\n",
    "\n",
    "The predictions made above assume that the observations $f(X_1) = \\mathbf{y}_1$ come from a noiseless distribution. We can notice this in the plot above because the posterior variance becomes zero at the observations $(X_1,\\mathbf{y}_1)$.\n",
    "We can make predictions from noisy observations $f(X_1) = \\mathbf{y}_1 + \\epsilon$, by modelling the noise $\\epsilon$ as Gaussian noise with variance $\\sigma_\\epsilon^2$.\n",
    "\n",
    "This noise can be modelled by adding it to the covariance kernel of our observations:\n",
    "\n",
    "$$\n",
    "\\Sigma_{11} = k(X_1,X_1) + \\sigma_\\epsilon^2 I\n",
    "$$\n",
    "\n",
    "Where $I$ is the identity matrix. Note that the noise only changes kernel values on the diagonal (white noise is independently distributed). The Gaussian process posterior with noisy observations is implemented in the `GP_noise` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process posterior with noisy obeservations\n",
    "def GP_noise(X1, y1, X2, kernel_func, σ_noise):\n",
    "    \"\"\"\n",
    "    Calculate the posterior mean and covariance matrix for y2\n",
    "    based on the corresponding input X2, the noisy observations \n",
    "    (y1, X1), and the prior kernel function.\n",
    "    \"\"\"\n",
    "    # Kernel of the noisy observations\n",
    "    Σ11 = kernel_func(X1, X1) + ((σ_noise ** 2) * np.eye(n1))\n",
    "    # Kernel of observations vs to-predict\n",
    "    Σ12 = kernel_func(X1, X2)\n",
    "    # Solve\n",
    "    solved = scipy.linalg.solve(Σ11, Σ12, assume_a='pos').T\n",
    "    # Compute posterior mean\n",
    "    μ2 = solved @ y1\n",
    "    # Compute the posterior covariance\n",
    "    Σ22 = kernel_func(X2, X2)\n",
    "    Σ2 = Σ22 - (solved @ Σ12)\n",
    "    return μ2, Σ2  # mean, covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calculates the posterior distribution of the previous 8 samples with added noise. Note in the plots that the variance $\\sigma_{2|1}^2$ at the observations is no longer 0, and that the functions sampled don't necessarily have to go through these observational points anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the posterior mean and covariance\n",
    "\n",
    "σ_noise = 1.  # The standard deviation of the noise\n",
    "# Add noise kernel to the samples we sampled previously\n",
    "y1 = y1 + ((σ_noise ** 2) * np.random.randn(n1))\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "μ2, Σ2 = GP_noise(X1, y1, X2, exponentiated_quadratic, σ_noise)\n",
    "# Compute the standard deviation at the test points to be plotted\n",
    "σ2 = np.sqrt(np.diag(Σ2))\n",
    "\n",
    "# Draw some samples of the posterior\n",
    "y2 = np.random.multivariate_normal(mean=μ2, cov=Σ2, size=ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the postior distribution and some samples\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(6, 6))\n",
    "# Plot the distribution of the function (mean, covariance)\n",
    "ax1.plot(X2, f_sin(X2), 'b--', label='$sin(x)$')\n",
    "ax1.fill_between(X2.flat, μ2-2*σ2, μ2+2*σ2, color='red', \n",
    "                 alpha=0.15, label=r'$2\\sigma_{2|1}$')\n",
    "ax1.plot(X2, μ2, 'r-', lw=2, label=r'$\\mu_{2|1}$')\n",
    "ax1.plot(X1, y1, 'ko', linewidth=2, label='$(x_1, y_1)$')\n",
    "ax1.set_xlabel('$x$', fontsize=13)\n",
    "ax1.set_ylabel('$y$', fontsize=13)\n",
    "ax1.set_title('Distribution of posterior and prior data')\n",
    "ax1.axis([domain[0], domain[1], -3, 3])\n",
    "ax1.legend()\n",
    "# Plot some samples from this function\n",
    "ax2.plot(X2, y2.T, '-')\n",
    "ax2.set_xlabel('$x$', fontsize=13)\n",
    "ax2.set_ylabel('$y$', fontsize=13)\n",
    "ax2.set_title('5 different function realizations from posterior')\n",
    "ax1.axis([domain[0], domain[1], -3, 3])\n",
    "ax2.set_xlim([-6, 6])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude we've implemented a Gaussian process and illustrated how to make predictions using it's posterior distribution.\n",
    "\n",
    "Key points to take away are:\n",
    "* A Gaussian process is a distribution over functions fully specified by a mean and covariance function.\n",
    "* Every finite set of the Gaussian process distribution is a multivariate Gaussian.\n",
    "* The posterior predictions of a Gaussian process are weighted averages of the observed data where the weighting is based on the covariance and mean functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first post part of a series on Gaussian processes. You can read [how to fit a Gaussian process kernel in the follow-up post]({% post_url /blog/gaussian_process/2019-01-06-gaussian-process-kernel-fitting %}).\n",
    "\n",
    "1. [Understanding Gaussian processes (this)]({% post_url /blog/gaussian_process/2019-01-05-gaussian-process-tutorial %})\n",
    "2. [Fitting a Gaussian process kernel]({% post_url /blog/gaussian_process/2019-01-06-gaussian-process-kernel-fitting %})\n",
    "3. [Gaussian process kernels]({% post_url /blog/gaussian_process/2019-01-07-gaussian-process-kernels %})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sidenotes\n",
    "\n",
    "1. Observe in the plot of the 41D Gaussian marginal from the exponentiated quadratic prior that the functions drawn from the Gaussian process distribution can be non-linear. The non-linearity is because the kernel can be interpreted as implicitly computing the inner product in a different space than the original input space (e.g. a higher dimensional feature space). This is what is commonly known as the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick).\n",
    "2. $\\Sigma_{11}^{-1} \\Sigma_{12}$ can be computed with the help of Scipy's [`solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html) function, which solves for $x$ the [linear system](https://en.wikipedia.org/wiki/System_of_linear_equations) $\\Sigma_{11} \\cdot x = \\Sigma_{12}$. Using this method [improves](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/) the speed and numerical accuracy compared to computing the inverse of $\\Sigma_{11}$ directly. Especially since it can make use of the fact that $\\Sigma_{11}$ is [symmetric and positive definite](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "1. [Introduction to Gaussian processes video lecture](https://www.youtube.com/watch?v=4vGiHC35j9s&list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&index=8) by Nando de Freitas.\n",
    "2. [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/) by Carl Edward Rasmussen and Christopher K. I. Williams (Book covering Gaussian processes in detail, online version downloadable as pdf).\n",
    "3. [Stochastic Processes and Applications](http://wwwf.imperial.ac.uk/~pavl/PavliotisBook.pdf) by Grigorios A. Pavliotis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python package versions used\n",
    "#%load_ext watermark\n",
    "#%watermark --python\n",
    "#%watermark --iversions\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post at <a rel=\"canonical\" href=\"https://peterroelants.github.io/posts/gaussian-process-tutorial/\">peterroelants.github.io</a> is generated from an Python notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/main/notebooks/gaussian_process/gaussian-process-tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
